# -*- coding: utf-8 -*-
"""Time_Series_Forecasting_Weather_in_Jena.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vgtNXcr8Dx90heS643Sqcib0tXloIFvl

# Perkenalan

Halo Kak Reviewer! Perkenalkan saya Rafka Imanda Putra dengan username dicoding rafka_imanda.

Dikesempatan kali ini saya akan mengirim submission kedua saya dalam Kelas Belajar Pengembangan Machine Learning.

Adapun dataset yang digunakan bersumber dari https://www.kaggle.com/mnassrib/jena-climate dimana ini adalah dataset yang berisi mengenai 420.551 data cuaca di Jena, Jerman berdasarkan periode dari tahun 2009 - 2016.

Tujuan saya disini adalah membuat model yang mampu memprediksi cuaca dengan Mean Absolute Error dibawah 10% dari skala data 

Mungkin cukup sekian, mari kita mulai pemodelannya!

# Data Understanding

Context --
Jena Climate is weather timeseries dataset recorded at the Weather Station of the Max Planck Institute for Biogeochemistry in Jena, Germany.

Content --
Jena Climate dataset is made up of 14 different quantities (such air temperature, atmospheric pressure, humidity, wind direction, and so on) were recorded every 10 minutes, over several years. This dataset covers data from January 1st 2009 to December 31st 2016.

# Data Preparation
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

# %matplotlib inline

import io
import warnings
from google.colab import files

warnings.filterwarnings('ignore')

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense

"""File CSV yang saya gunakan, sudah menghapus beberapa kolom yang tidak digunakan, demi menghemat space saat menguploadnya kesini."""

file = files.upload()

df = pd.read_csv(io.BytesIO(file['jena_climate_2009_2016.csv']))

df.head()

df.shape

df.isnull().sum()

df.info()

df.describe()

df.columns = ['date', 'degree']

df['date'] = pd.to_datetime(df['date'])

"""## Resampling Data

Disini saya ingin mengubah data dari format YY:MM:DD:HH:MM menjadi YY:MM:DD:HH, dan valuenya dirata-ratakan. Selain itu, dengan melakukan resampling ini akan merubah banyaknya jumlah data menjadi sekitar 70.129 data point
"""

df1 = df.set_index('date').resample('H').mean()

df1 = df1.reset_index()
df1.head()

df1.describe()

df1.shape

df1.isnull().sum()

df1['degree'].fillna(df1['degree'].median(), inplace=True)

sns.histplot(df1['degree'], bins=60);

plt.figure(figsize=(25,5))
plt.plot(df1['date'].values, df1['degree'].values)
plt.title('Besaran Derajat (Celcius) Suhu Wilayah Jena di Breakdown per Jam selama Tahun 2009-2016', fontsize=16, pad=20)
plt.show()

"""# Splitting Validation Data"""

print(f'Jumlah Data sebanyak {df1.shape[0]}')
print('')
print('Kita ingin membagi dataset menjadi 80% dataset pertama sebagai train_dataset, dan 20% data terakhir sebagai validation_dataset')
print('Maka dengan asumsi itu.....')
print('')
print(f'80% Data Latih, artinya sebanyak {df1.shape[0] * (80 / 100)} Data Terawal')
print('Kita singkat menjadi 56.103 Data Terawal')
print('')
print(f'20% Data Validasi, artinya sebanyak {df1.shape[0] * (20 / 100)} Data Terakhir')
print('Kita singkat menjadi 14.026 Data Terakhir')

jumlah_train = 56103
jumlah_val = 14026 

jumlah_train + jumlah_val == df1.shape[0]

df_train = df1.iloc[0:jumlah_train, :].copy()
df_val = df1.iloc[-jumlah_val:, :].copy()

def windowed_dataset(series, window_size, batch_size, shuffle_buffer):
  series = tf.expand_dims(series, axis=-1)
  ds = tf.data.Dataset.from_tensor_slices(series)
  ds = ds.window(window_size + 1, shift=1, drop_remainder=True)
  ds = ds.flat_map(lambda x : x.batch(window_size + 1))
  ds = ds.shuffle(shuffle_buffer)
  ds = ds.map(lambda x : (x[:-1], x[-1:]))
  return ds.batch(batch_size).prefetch(1)

train_set = windowed_dataset(df_train['degree'].values, window_size=60, batch_size=100, shuffle_buffer=1000)
validation_set = windowed_dataset(df_val['degree'].values, window_size=60, batch_size=100, shuffle_buffer=1000)

"""# Modelling"""

class myCallback(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if(logs.get('val_mae') < 0.4):
      print('\nYeay, mean absolute error memenuhi target!')
      self.model.stop_training = True

callbacks = myCallback()

model = Sequential([
                    LSTM(60, return_sequences=True),
                    LSTM(60),
                    Dense(30, activation='relu'),
                    Dense(10, activation='relu'),
                    Dense(1)
])

optimizer = tf.keras.optimizers.SGD(lr=1.0000e-04, momentum=0.9)
model.compile(loss=tf.keras.losses.Huber(),
              optimizer=optimizer,
              metrics=['mae'])

history = model.fit(train_set, epochs=20, validation_data=(validation_set), callbacks=[callbacks])

"""# Plot Loss dan Metrik MAE"""

plt.figure(figsize=(8,5))
plt.plot(history.history['mae'], label='train_mae')
plt.plot(history.history['val_mae'], label='validation_mae')
plt.title('Model MAE')
plt.xlabel('Epochs')
plt.ylabel('MAE')
plt.legend()
plt.ylim(ymin=0, ymax=10)
plt.show()

plt.figure(figsize=(8,5))
plt.plot(history.history['loss'], label='train_loss')
plt.plot(history.history['val_loss'], label='validation_loss')
plt.title('Model Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.ylim(ymin=0)
plt.show()

print(df1['degree'].min(), df1['degree'].max())
print('Range Data = {}'.format(np.abs(df1['degree'].min()) + df1['degree'].max()))

# Mencari Skala Data

from sklearn.preprocessing import MinMaxScaler

data = np.array([df1['degree'].min(), df1['degree'].max(), df['degree'].min() + 0.6959]).reshape(-1,1)   # 0,6959 adalah hasil MAE data validasi
scaler = MinMaxScaler()

scaler.fit_transform(data)

"""Berdasarkan hasil normalisasi hasil MAE dari data validasi, model ini memiliki MAE < 10% skala data.

# Conclusion

Dataset yang akan dipakai bebas, namun minimal memiliki 1000 sampel. -- DONE

Harus menggunakan LSTM dalam arsitektur model. -- DONE

Validation set sebesar 20% dari total dataset. -- DONE

Model harus menggunakan model sequential. -- DONE

Harus menggunakan Learning Rate pada Optimizer. -- DONE

Dataset yang digunakan memiliki banyak sampel data. -- DONE

Mengimplementasikan Callback. -- DONE

Membuat plot loss dan akurasi pada saat training dan validation. -- DONE

dataset memiliki minimal 10000 sampel data. -- DONE

MAE dari model < 10% skala data. -- DONE
"""